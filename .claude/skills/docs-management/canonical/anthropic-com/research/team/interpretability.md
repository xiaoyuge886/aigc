---
source_url: https://www.anthropic.com/research/team/interpretability
source_type: sitemap
content_hash: sha256:984be468c09eaa1906fde1d7a03a94ab040aeccf11d7a8e7985a0babedb5b36b
sitemap_url: https://www.anthropic.com/sitemap.xml
fetch_method: html
---

[Back to Overview](/research)

# Interpretability

[![Video](https://img.youtube.com/vi/Bj9BD2D3DzA/maxresdefault.jpg)](https://www.youtube.com/watch?v=Bj9BD2D3DzA)

[![Video](https://img.youtube.com/vi/TxhhMTOTMDg/maxresdefault.jpg)](https://www.youtube.com/watch?v=TxhhMTOTMDg)

The mission of the Interpretability team is to discover and understand how large language models work internally, as a foundation for AI safety and positive outcomes.

Research teams:[Alignment](/research/team/alignment)[Economic Research](/research/team/economic-research)[Interpretability](/research/team/interpretability)[Societal Impacts](/research/team/societal-impacts)

![Video thumbnail](https://cdn.sanity.io/images/4zrzovbb/website/27748c7d4c470d931ef828085ecb870d6a41f0e9-1440x810.png)

### Safety through understanding

It's very challenging to reason about the safety of neural networks without understanding them. The Interpretability team’s goal is to be able to explain large language models’ behaviors in detail, and then use that to solve a variety of problems ranging from bias to misuse to autonomous harmful behavior.

### Multidisciplinary approach

Some Interpretability researchers have deep backgrounds in machine learning – one member of the team is often described as having started mechanistic interpretability, while another was on the famous scaling laws paper. Other members joined after careers in astronomy, physics, mathematics, biology, data visualization, and more.

![Video thumbnail](https://cdn.sanity.io/images/4zrzovbb/website/6c30d7fb336b69fc7756b9846ee88c8d0d5122da-1920x1080.png)

[## Tracing the thoughts of a large language model

InterpretabilityMar 27, 2025

Circuit tracing lets us watch Claude think, uncovering a shared conceptual space where reasoning happens before being translated into language—suggesting the model can learn something in one language and apply it in another.](/research/tracing-thoughts-language-model)

[InterpretabilityOct 29, 2025

#### Signs of introspection in large language models

Can Claude access and report on its own internal states? This research finds evidence for a limited but functional ability to introspect—a step toward understanding what's actually happening inside these models.](/research/introspection)[InterpretabilityAug 1, 2025

#### Persona vectors: Monitoring and controlling character traits in language models

AI models represent character traits as patterns of activations within their neural networks. By extracting "persona vectors" for traits like sycophancy or hallucination, we can monitor personality shifts and mitigate undesirable behaviors.](/research/persona-vectors)[InterpretabilitySep 14, 2022

#### Toy Models of Superposition

Neural networks pack many concepts into single neurons. This paper shows how and when models represent more features than they have dimensions.](/research/toy-models-of-superposition)

## Publications

Search

DateCategoryTitle

* [Oct 29, 2025Interpretability

  Signs of introspection in large language models](/research/introspection)
* [Aug 1, 2025Interpretability

  Persona vectors: Monitoring and controlling character traits in language models](/research/persona-vectors)
* [May 29, 2025Interpretability

  Open-sourcing circuit tracing tools](/research/open-source-circuit-tracing)
* [Mar 27, 2025Interpretability

  Tracing the thoughts of a large language model](/research/tracing-thoughts-language-model)
* [Mar 13, 2025Alignment

  Auditing language models for hidden objectives](/research/auditing-hidden-objectives)
* [Feb 20, 2025Interpretability

  Insights on Crosscoder Model Diffing](/research/crosscoder-model-diffing)
* [Oct 25, 2024Societal Impacts

  Evaluating feature steering: A case study in mitigating social biases](/research/evaluating-feature-steering)
* [Oct 16, 2024Interpretability

  Using dictionary learning features as classifiers](/research/features-as-classifiers)
* [Oct 1, 2024Interpretability

  Circuits Updates – September 2024](/research/circuits-updates-sept-2024)
* [Sep 6, 2024Interpretability

  Circuits Updates – August 2024](/research/circuits-updates-august-2024)

[See more](#)

![Signs of introspection in large language models](https://www-cdn.anthropic.com/images/4zrzovbb/website/46e4aa7ea208ed440d5bd9e9e3a0ee66bc336ff1-1000x1000.svg)

Join the Research team

[See open roles](/jobs)
