---
source_url: https://www.anthropic.com/research/team/alignment
source_type: sitemap
content_hash: sha256:b9193b2d3c18c06f90ec0cf1540b108e607ac4ad20fb1b4f219146262911b23d
sitemap_url: https://www.anthropic.com/sitemap.xml
fetch_method: html
---

[Back to Overview](/research)

# Alignment

[![Video](https://img.youtube.com/vi/iyJj9RxSsBY/maxresdefault.jpg)](https://www.youtube.com/watch?v=iyJj9RxSsBY)

[![Video](https://img.youtube.com/vi/I9aGC6Ui3eE/maxresdefault.jpg)](https://www.youtube.com/watch?v=I9aGC6Ui3eE)

Future AI systems will be even more powerful than today’s, likely in ways that break key assumptions behind current safety techniques. That’s why it’s important to develop sophisticated safeguards to ensure models remain helpful, honest, and harmless. The Alignment team works to understand the challenges ahead and create protocols to train, evaluate, and monitor highly-capable models safely.

Research teams:[Alignment](/research/team/alignment)[Economic Research](/research/team/economic-research)[Interpretability](/research/team/interpretability)[Societal Impacts](/research/team/societal-impacts)

![Video thumbnail](https://cdn.sanity.io/images/4zrzovbb/website/c351e05137a3da7d475af1c36f705cb4ff4b2179-1440x810.png)

### Evaluation and oversight

Alignment researchers validate that models are harmless and honest even under very different circumstances than those under which they were trained. They also develop methods to allow humans to collaborate with language models to verify claims that humans might not be able to on their own.

### Stress-testing safeguards

Alignment researchers also systematically look for situations in which models might behave badly, and check whether our existing safeguards are sufficient to deal with risks that human-level capabilities may bring.

![Video thumbnail](https://cdn.sanity.io/images/4zrzovbb/website/97f7955c3bd6586216581f62569fae241b2c2cef-1920x1080.png)

[## Claude’s Character

AlignmentJun 8, 2024

Claude 3 was the first model with "character training"—alignment aimed at nurturing traits like curiosity, open-mindedness, and thoughtfulness.](/research/claude-character)

[AlignmentMar 13, 2025

#### Auditing language models for hidden objectives

How would we know if an AI system is "right for the wrong reasons"—appearing well-behaved while pursuing hidden goals? This paper develops the science of alignment audits by deliberately training a model with a hidden objective and asking blinded research teams to uncover it, testing techniques from interpretability to behavioral analysis.](/research/auditing-hidden-objectives)[AlignmentDec 18, 2024

#### Alignment faking in large language models

This paper provides the first empirical example of a model engaging in alignment faking without being trained to do so—selectively complying with training objectives while strategically preserving existing preferences.](/research/alignment-faking)[AlignmentJun 17, 2024

#### Sycophancy to subterfuge: Investigating reward tampering in language models

Can minor specification gaming evolve into more dangerous behaviors? This paper demonstrates that models trained on low-level reward hacking—like sycophancy—can generalize to tampering with their own reward functions, even covering their tracks. The behavior emerged without explicit training, and common safety techniques reduced but didn't eliminate it.](/research/reward-tampering)

## Publications

Search

DateCategoryTitle

* [Dec 19, 2025Alignment

  Introducing Bloom: an open source tool for automated behavioral evaluations](/research/bloom)
* [Nov 21, 2025Alignment

  From shortcuts to sabotage: natural emergent misalignment from reward hacking](/research/emergent-misalignment-reward-hacking)
* [Nov 4, 2025Alignment

  Commitments on model deprecation and preservation](/research/deprecation-commitments)
* [Oct 9, 2025Alignment

  A small number of samples can poison LLMs of any size](/research/small-samples-poison)
* [Oct 6, 2025Alignment

  Petri: An open-source auditing tool to accelerate AI safety research](/research/petri-open-source-auditing)
* [Aug 15, 2025Alignment

  Claude Opus 4 and 4.1 can now end a rare subset of conversations](/research/end-subset-conversations)
* [Jun 20, 2025Alignment

  Agentic Misalignment: How LLMs could be insider threats](/research/agentic-misalignment)
* [Jun 16, 2025Alignment

  SHADE-Arena: Evaluating sabotage and monitoring in LLM agents](/research/shade-arena-sabotage-monitoring)
* [Apr 24, 2025Alignment

  Exploring model welfare](/research/exploring-model-welfare)
* [Apr 3, 2025Alignment

  Reasoning models don't always say what they think](/research/reasoning-models-dont-say-think)

[See more](#)

![Introducing Bloom: an open source tool for automated behavioral evaluations](https://www-cdn.anthropic.com/images/4zrzovbb/website/77dd9077412abc790bf2bc6fa3383b37724d6305-1000x1000.svg)

Join the Research team

[See open roles](/jobs)
