---
source_url: https://www.anthropic.com/news/compliance-framework-SB53
source_type: sitemap
content_hash: sha256:fd62da46b9088dc19ccd7709fe3b1c275ba83860a87b95533891c94be724d666
sitemap_url: https://www.anthropic.com/sitemap.xml
fetch_method: html
published_at: '2025-12-19'
---

Policy

# Sharing our compliance framework for California's Transparency in Frontier AI Act

Dec 19, 2025

![](https://www-cdn.anthropic.com/images/4zrzovbb/website/6e00dbffcddc82df5e471c43453abfc74ca94e8d-1000x1000.svg)

On January 1, California's Transparency in Frontier AI Act ([SB 53](https://leginfo.legislature.ca.gov/faces/billNavClient.xhtml?bill_id=202520260SB53)) will go into effect. It establishes the nation’s first frontier AI safety and transparency requirements for catastrophic risks.

While we have long advocated for a federal framework, Anthropic [endorsed](https://www.anthropic.com/news/anthropic-is-endorsing-sb-53) SB 53 because we believe frontier AI developers like ourselves should be transparent about how they assess and manage these risks. Importantly, the law balances the need for strong safety practices, incident reporting, and whistleblower protections—while preserving flexibility in how developers implement their safety measures, and exempting smaller companies from unnecessary regulatory burdens.

One of the law’s key requirements is that frontier AI developers publish a framework describing how they assess and manage catastrophic risks. Our Frontier Compliance Framework (FCF) is now available to the public, [here](https://trust.anthropic.com/resources?s=eorilovp4wxk38nxbi7k3&name=anthropic-frontier-compliance-framework). Below, we discuss what’s included within it, and highlight what we think should come next for frontier AI transparency.

## **What’s in our Frontier Compliance Framework**

Our FCF describes how we assess and mitigate cyber offense, chemical, biological, radiological, and nuclear threats, as well as the risks of AI sabotage and loss of control, for our frontier models. The framework also lays out our tiered system for evaluating model capabilities against these risk categories and explains our approach to mitigations. It also covers how we protect model weights and respond to safety incidents.

Much of what's in the FCF reflects an evolution of practices we've followed for years. Since 2023, our [Responsible Scaling Policy](https://www.anthropic.com/news/anthropics-responsible-scaling-policy) (RSP) has outlined our approach to managing extreme risks from advanced AI systems and informed our decisions about AI development and deployment. We also release detailed system cards when we launch new models, which describe capabilities, safety evaluations, and risk assessments. Other labs have voluntarily adopted similar approaches. Under the new law going into effect on January 1, those types of transparency practices are mandatory for those building the most powerful AI systems in California.

Moving forward, the FCF will serve as our compliance framework for SB 53 and other regulatory requirements. The RSP will remain our voluntary safety policy, reflecting what we believe best practices should be as the AI landscape evolves, even when that goes beyond or otherwise differs from current regulatory requirements.

## **The need for a federal standard**

The implementation of SB 53 is an important moment. By formalizing achievable transparency practices that responsible labs already voluntarily follow, the law ensures these commitments can't be abandoned quietly later once models get more capable, or as competition intensifies. Now, a federal AI transparency framework enshrining these practices is needed to ensure consistency across the country.

Earlier this year, we proposed a [framework](https://www.anthropic.com/news/the-need-for-transparency-in-frontier-ai) for federal legislation. It emphasizes public visibility into safety practices, without trying to lock in specific technical approaches that may not make sense over time. The core tenets of our framework include:

* **Requiring a public secure development framework:** Covered developers should publish a framework laying out how they assess and mitigate serious risks, including chemical, biological, radiological, and nuclear harms, as well as harms from misaligned model autonomy.
* **Publishing system cards at deployment:** Documentation summarizing testing, evaluation procedures, results, and mitigations should be publicly disclosed when models are deployed and updated if models are substantially modified.
* **Protecting whistleblowers**: It should be an explicit violation of law for a lab to lie about compliance with its framework or punish employees who raise concerns about violations.
* **Flexible transparency standards:** A workable AI transparency framework should have a minimum set of standards so that it can enhance security and public safety while accommodating the evolving nature of AI development. Standards should be flexible, lightweight requirements that can adapt as consensus best practices emerge.
* **Limit application to the largest model developers**: To avoid burdening the startup ecosystem and smaller developers with models at low risk for causing catastrophic harm, requirements should apply only to established frontier developers building the most capable models.

As AI systems grow more powerful, the public deserves visibility into how they're being developed and what safeguards are in place. We look forward to working with Congress and the administration to develop a national transparency framework that ensures safety while preserving America’s AI leadership.


<!-- Content filtered: site navigation/footer -->
